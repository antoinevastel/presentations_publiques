{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'images\n",
    "\n",
    "Cet article présente comment utiliser un réseau de neurones pré-entrainé pour résoudre un problème de classification d’images (tâche différente que celle pour laquelle le réseau a été préalablement entrainé). Dans notre cas, l’objectif est de classer des images satellites de toits en 4 catégories : orientation EST/OUEST, orientation NORD/SUD, toit plat, et catégorie “Autre”. Il est important de préciser une nouvelle fois que cette tâche de classification est différente de celle pour laquelle le réseau de neuronnes que nous allons utiliser a été entrainé.\n",
    "\n",
    "La librairie de réseau de neurones que nous utiliserons se nomme MxNET. Elle est disponible à la fois sous Linux et Windows, et possède (entre autre) un wrapper en Python et en R. Dans cet article le code a été exécuté sous Ubuntu avec le wrapper Python.\n",
    "\n",
    "# Installation de la librairie sous Ubuntu\n",
    "Commencer par exécuter les commandes suivantes dans un terminal afin d'installer les dépendances :\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "sudo apt-get install -y build-essential git libatlas-base-dev libopencv-dev\n",
    "\n",
    "\n",
    "Ensuite, clonez le dépot Github de MxNet grâce à la commande ci-après :\n",
    "\n",
    "git clone --recursive https://github.com/dmlc/mxnet\n",
    "\n",
    "\n",
    "Puis pour compiler la librairie : \n",
    "\n",
    "cd mxnet; make -j$(nproc)\n",
    "\n",
    "\n",
    "Si vous disposez d'un GPU assez récent et que vous avez déjà installé les librairies CUDA suffisantes, vous pouvez compiler la librairie avec la commande ci-dessous. Cela permettra à MxNet de tirer partie de votre GPU.\n",
    "\n",
    "cd mxnet;make -j4 USE_CUDA=1\n",
    "\n",
    "\n",
    "Enfin, pour installer le package Python : \n",
    "\n",
    "cd python; sudo python setup.py install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des features\n",
    "\n",
    "La première étape consiste à utiliser un réseau de neurones déjà entrainé afin d'extraire des features de nos images. Pour cela nous utilisons un réseau de neurones nommés Inception-BN. BN vient du fait qu'il a été entrainé en utilisant une technique ce nomment batch normalization.\n",
    "Il a été entrainé sur le dataset ILSVRC2012 sur lequel il atteint une accuracy top-1 de 72.5% et top-5 de 90.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports relatifs a l'extraction de features via le CNN\n",
    "import mxnet as mx\n",
    "import logging\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "from skimage.util import img_as_float\n",
    "from skimage.transform import resize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(path, show_img=False, mean_img=None):\n",
    "    # Loads the image\n",
    "    img = io.imread(path)\n",
    "    short_egde = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_egde) / 2)\n",
    "    xx = int((img.shape[1] - short_egde) / 2)\n",
    "    crop_img = img[yy : yy + short_egde, xx : xx + short_egde]\n",
    "    # Resizes image to 224, 224\n",
    "    resized_img = transform.resize(crop_img, (224, 224))\n",
    "    if show_img:\n",
    "        io.imshow(resized_img)\n",
    "    sample = np.asarray(resized_img) * 256\n",
    "    # Swaps axis of the image to transform it from (224, 224, 4) to (3, 224, 224)\n",
    "    sample = np.swapaxes(sample, 0, 2)\n",
    "    sample = np.swapaxes(sample, 1, 2)\n",
    "    # We substract the mean\n",
    "    normed_img = sample - mean_img.asnumpy()\n",
    "    normed_img.resize(1, 3, 224, 224)\n",
    "    return normed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/id_train.csv\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "#On charge le reseau pre entraine\n",
    "prefix = \"./model/Inception_BN\"\n",
    "num_round = 39\n",
    "model = mx.model.FeedForward.load(prefix , num_round, ctx=mx.cpu(), numpy_batch_size=1)\n",
    "\n",
    "#on charge l'image moyenne\n",
    "mean_img = mx.nd.load(\"./model/mean_224.nd\")[\"mean_img\"]\n",
    "f = open(\"./features_train.csv\", 'w')\n",
    "internals = model.symbol.get_internals()\n",
    "fea_symbol = internals[\"global_pool_output\"]\n",
    "feature_extractor = mx.model.FeedForward(ctx=mx.cpu(), symbol=fea_symbol, numpy_batch_size=1,\n",
    "                             arg_params=model.arg_params, aux_params=model.aux_params,\n",
    "                             allow_extra_params=True)\n",
    "\n",
    "# column0 = image id, column 1= label, other columns = image features\n",
    "for index, row in df.iterrows():\n",
    "    imagePath = \"./data/roof_images/\"+str(row[\"Id\"])+\".jpg\"\n",
    "    batch = preprocess_image(imagePath, False, mean_img)\n",
    "    # on extrait les features de l'image\n",
    "    global_pooling_feature = feature_extractor.predict(batch)\n",
    "    # on sauvegarde les features dans un fichier\n",
    "    f.write(str(row[\"Id\"])+\",\"+str(row[\"label\"]))\n",
    "    f.write(\",\")\n",
    "    for i in range(0, len(global_pooling_feature[0])-1):\n",
    "        f.write(str(global_pooling_feature[0][i][0][0]))\n",
    "        f.write(\",\")\n",
    "    f.write(str(global_pooling_feature[0][len(global_pooling_feature[0])-1][0][0]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur supervisé\n",
    "\n",
    "L'objectif est d'entraîner un classifieur supervisé, ici une SVM, grâce aux features extraites précédemment afin de classifier nos images dans l'une des 4 catégories : nord/sud, est/ouest, toit plat, autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avastel/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./features_train.csv\")\n",
    "train, test = train_test_split(df, train_size = 0.8, test_size = 0.2)\n",
    "X_train = train.ix[:,2:]\n",
    "y_train = train.ix[:,1]\n",
    "X_test = test.ix[:, 2:]\n",
    "y_test = test.ix[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='accuracy', n_jobs=4)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on development set:\n",
      "\n",
      "0.747 (+/-0.022) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 1}\n",
      "0.679 (+/-0.032) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 1}\n",
      "0.762 (+/-0.014) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n",
      "0.746 (+/-0.016) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 10}\n",
      "0.746 (+/-0.007) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "0.750 (+/-0.015) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 100}\n",
      "0.745 (+/-0.008) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 1000}\n",
      "0.724 (+/-0.016) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 1000}\n",
      "0.693 (+/-0.015) for {'kernel': 'linear', 'C': 1}\n",
      "0.670 (+/-0.023) for {'kernel': 'linear', 'C': 10}\n",
      "0.667 (+/-0.018) for {'kernel': 'linear', 'C': 100}\n",
      "0.667 (+/-0.018) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Accuracy : 0.766875 \n"
     ]
    }
   ],
   "source": [
    "print(\"Grid scores on development set:\\n\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\\n\")\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The score is computed on the full evaluation set.\\n\")\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %f \" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the svm with best hyperparameters)\n",
    "model = svm.SVC(kernel=\"rbf\", C=10.0, gamma=0.001)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.770625 \n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of our model\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy : %f \" % accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
